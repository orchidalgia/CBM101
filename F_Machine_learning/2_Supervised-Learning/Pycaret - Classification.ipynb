{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ce4b83",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b211e9",
   "metadata": {},
   "source": [
    "### What is Binary Classification?\n",
    "Binary classification is a supervised machine learning technique where the goal is to predict categorical class labels which are discrete and unoredered such as Pass/Fail, Positive/Negative, Default/Not-Default etc. A few real world use cases for classification are listed below:\n",
    "\n",
    "Medical testing to determine if a patient has a certain disease or not - the classification property is the presence of the disease.\n",
    "A \"pass or fail\" test method or quality control in factories, i.e. deciding if a specification has or has not been met – a go/no-go classification.\n",
    "Information retrieval, namely deciding whether a page or an article should be in the result set of a search or not – the classification property is the relevance of the article, or the usefulness to the user.\n",
    "\n",
    "#### Pycaret classification\n",
    "\n",
    "PyCaret's classification module `pycaret.classification` is a supervised machine learning module which is used for classifying the elements into groups based on various techniques and algorithms. The PyCaret classification module can be used for Binary or Multi-class classification problems. It has over 18 algorithms and 14 plots to analyze the performance of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb30364",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962ce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.datasets import get_data\n",
    "from pycaret.classification import *\n",
    "from pycaret.utils import check_metric\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1590f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452f768",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9370bb",
   "metadata": {},
   "source": [
    "Pycaret's repository has a lot of easily accessible datasets that we can load using `get_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all datasets available\n",
    "get_data('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d88de",
   "metadata": {},
   "source": [
    "### Heart disease dataset\n",
    "\n",
    "For this tutorial we will use a dataset from UCI called *Heart disease data set*. This dataset contains information of potential heart disease patients from 1988. The target is to classify if the patient has heart disease or not (column `Disease`) There are 270 samples and 14 features. Short descriptions of each column are as follows:\n",
    "\n",
    "**age** = age in years </br>\n",
    "**sex** = 1=male, 0=female </br>\n",
    "**chest_pain_type** = chest pain type </br>\n",
    "-- Value 1: typical angina </br>\n",
    "-- Value 2: atypical angina </br>\n",
    "-- Value 3: non-anginal pain </br>\n",
    "-- Value 4: asymptomatic </br>\n",
    "**resting_blood_pressure** = resting blood pressure in mmHg  </br>\n",
    "**serum_cholestoral_in_mg/dl** = serum cholestoral in mg/dl </br>\n",
    "**fasting_blood_sugar_>_120_mg/dl** = 1=true, 0=false </br>\n",
    "**resting_electrocardiographic_results** =  </br>\n",
    "-- Value 0: normal </br>\n",
    "-- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) </br>\n",
    "-- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria </br>\n",
    "**maximum_heart_rate_achieved** =  </br>\n",
    "**exercise_induced_angina** = 1=true, 0=false </br>\n",
    "**oldpeak** = ST depression induced by exercise relative to rest </br>\n",
    "**slope_of_peak** = the slope of the peak exercise ST segment </br>\n",
    "-- Value 1: upsloping </br>\n",
    "-- Value 2: flat </br>\n",
    "-- Value 3: downsloping </br>\n",
    "**number_of_major_vessels** = number of major vessels (0-3) colored by flourosopy </br>\n",
    "**thal** = 3 = normal; 6 = fixed defect; 7 = reversable defect </br>\n",
    "**Disease** =  </br>\n",
    "-- Value 0: < 50% diameter narrowing, no disease </br>\n",
    "-- Value 1: > 50% diameter narrowing, disease </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36359f76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = get_data('heart_disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473bf97",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<h4>Exercise 1. Column names have spaces between words and after the names as well. Delete all spaces after column names and then replace all remaning spaces with underscores.</h4>\n",
    "</div>\n",
    "\n",
    "**Hint**: Use `strip()` and `replace()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce5a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pyc_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49445a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba2190",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Split data into a [Training set and a Test set](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "In supervised machine learning, a key question regards the evaluation of models - how good are they? Just like statistics uses things like $R^2$ as a goodness-of-fit measure, the machine learning pipeline is equipped with methods of measuring a model's quality. To do this without a bias, we cannot test the model on the same data it was trained on - but on a separate test set. We therefore split the data like below.\n",
    "\n",
    "**75% for training and 25% for testing (by default)** \n",
    "\n",
    "Fix a `random_state` for reproducibility: `train_test_split` splits arrays or matrices into random train and test subsets. That means that everytime you run it without specifying random_state, you will get a different result. If you seed your random number generator with a fixed number (say 42), your split will be always the same. \n",
    "![title](assets/Train_test_sets.jpg)\n",
    "\n",
    "**Pycaret will do the splitting automatically but we can still save part of the data to test on the finalized model. This way we can test the finalized model using completely new unseen data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset to data and target\n",
    "data = df.loc[:, df.columns != 'Disease']\n",
    "target = df['Disease']\n",
    "\n",
    "# split to train and test. Set test_size to 0.05 since pycaret will automatically do the 75/25 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, random_state=42, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1339c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5511a",
   "metadata": {},
   "source": [
    "## Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcdcfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy function for plotting features\n",
    "def boxplot(data, x, y):\n",
    "    plt.figure(figsize = (10,7))\n",
    "\n",
    "    # Usual boxplot\n",
    "    ax = sns.boxplot(x=x, y=y, data=data)\n",
    " \n",
    "    # Add jitter with the swarmplot function.\n",
    "    ax = sns.swarmplot(x=x, y=y, data=data, color=\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52778449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot age and target\n",
    "boxplot(df, target, 'age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c679a",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "**Normalization**: Normalization / Scaling (often used interchangeably with standardization) is used to transform the actual values of numeric variables in a way that provides helpful properties for machine learning. Many algorithms such as Logistic Regression, Support Vector Machine, K Nearest Neighbors and Naive Bayes assume that all features are centered around zero and have variances that are at at the same level of order. If a particular feature in a dataset has a variance that is larger in order of magnitude than other features, the model may not understand all features correctly and could perform poorly. For instance, in the dataset we are using for this example the AGE feature ranges between 29 to 77 while other numeric features like cholesterol range from 126 to 564. <a href=\"https://sebastianraschka.com/Articles/2014_about_feature_scaling.html#z-score-standardization-or-min-max-scaling\">Read more</a>\n",
    "\n",
    "**Transformation**: While normalization transforms the range of data to remove the impact of magnitude in variance, transformation is a more radical technique as it changes the shape of the distribution so that transformed data can be represented by a normal or approximate normal distirbution. In general, you should transform the data if using algorithms that assume normality or a gaussian distribution. Examples of such models are Logistic Regression, Linear Discriminant Analysis (LDA) and Gaussian Naive Bayes. (Pro tip: any method with “Gaussian” in the name probably assumes normality.) <a href=\"https://en.wikipedia.org/wiki/Power_transform\">Read more</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a55e2",
   "metadata": {},
   "source": [
    "## Setting up Environment in PyCaret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bbc983",
   "metadata": {},
   "source": [
    "### Classification in Pycaret\n",
    "\n",
    "`setup()` is Pycaret's main function and it needs to be run before executing any other function in pycaret. The `setup()` function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment.\n",
    "\n",
    "We'll set normalize and transformation *True* for automatic preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8161ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# `session_id` parameter is equivalent to ‘random_state’ in scikit-learn. Let's use 42 for reproducibility.\n",
    "s = setup(X_train, target=y_train, normalize = True, transformation = True, session_id=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e5867",
   "metadata": {},
   "source": [
    "Once the setup has been succesfully executed it prints the information grid which contains several important pieces of information. Most of the information is related to the pre-processing pipeline which is constructed when setup() is executed. The majority of these features are out of scope for the purposes of this tutorial however a few important things to note at this stage include:\n",
    "\n",
    "**session_id** : A pseduo-random number distributed as a seed in all functions for later reproducibility. If no session_id is passed, a random number is automatically generated that is distributed to all functions. In this experiment, the session_id is set as 42 for later reproducibility.\n",
    "\n",
    "**Target Type** : Binary or Multiclass. The Target type is automatically detected and shown. There is no difference in how the experiment is performed for Binary or Multiclass problems. All functionalities are identical.\n",
    "\n",
    "**Original Data** : Displays the original shape of the dataset. In this experiment (256, 14) means 256 samples and 14 features including the target column.\n",
    "\n",
    "**Transformed Train Set** : Displays the shape of the transformed training set. Notice that the data is splitted into (179, 14).\n",
    "\n",
    "**Transformed Test Set** : Displays the shape of the transformed test/hold-out set. There are 77 samples in test/hold-out set. This split is based on the default value of 70/30 that can be changed using the train_size parameter in setup.\n",
    "\n",
    "**Transformation method** : By default, the transformation method is set to ‘yeo-johnson’. The other available option for transformation is ‘quantile’. Can be changed using *transformation_method* parameter.\n",
    "\n",
    "**Normalize method** : By default, normalize method is set to ‘zscore’ The standard zscore is calculated as z = (x - u) / s. The other available option for normalizing is 'minmax'. Can be changed using *normalize_method* parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5190cdd",
   "metadata": {},
   "source": [
    "## Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93317cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list all ML models\n",
    "models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a2faf",
   "metadata": {},
   "source": [
    "Pycaret runs all different ML algorhitms using default parameters. We can compare all models using `compare_models()` which puts all models in order from best to worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4933d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best model is saved in best_model object\n",
    "best_model = compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340313f",
   "metadata": {},
   "source": [
    "This gives us lots of metrics we can use to evaluate the results:\n",
    "\n",
    "\n",
    "**Accuracy** = $ \\frac{Correctly\\:predicted}{Total\\:samples}$  <br>\n",
    "It's simple and informative but doesn't give us the whole picture since sometimes e.g. false negative predictions should be minimized. <br><br>\n",
    "\n",
    "**Precision** = $ \\frac{True\\:positive}{Total\\:predicted\\:positive}$ <br>\n",
    "Precision is a measure to determine, how precise model's positive predictions are. If model is trying to predict spam-emails (1), low precision means that rate of false positives is high i.e. lots of valid emails (0) are classified as spam.<br> <br>\n",
    "\n",
    "**Recall** = $ \\frac{True\\:positive}{Total\\:actually\\:positive}$ <br>\n",
    "In other words how many actual positives are classified correctly. Using the same example, spam emails leak into your inbox as normal emails if recall is low. <br><br>\n",
    "\n",
    "**F1** = $ 2* \\frac{Precision*Recall}{Precision+Recall}$ <br>\n",
    "F1 score tells the balance between precision and recall. It's similar to accuracy but takes false positives and negatives into account, making it better metric if false predictions are significant. In addition if class distribution is uneven (e.g lots of negatives and only few positives), model may predict all negative, making accuracy high but F1-score low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ad0ef",
   "metadata": {},
   "source": [
    "<img src=\"./assets/precisionrecall.png\" width=\"400\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d6ca5",
   "metadata": {},
   "source": [
    "Let's make few models using 10 fold stratified **cross validation**. You can change number of folds using `fold` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "lr = create_model('lr', round = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9aafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier \n",
    "rf = create_model('rf', round = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b19c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# k nearest neighbors\n",
    "knn = create_model('knn', round = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06607d6",
   "metadata": {},
   "source": [
    "## Plotting the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0934b",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "A confusion matrix summarizes the performance of a classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5684c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(rf, plot = 'confusion_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f1192",
   "metadata": {},
   "source": [
    "#### Error plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eefdf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(rf, plot = 'error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c5c20e",
   "metadata": {},
   "source": [
    "#### Boundary plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67af5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_model(rf, plot = 'boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4026c1a6",
   "metadata": {},
   "source": [
    "#### Feature plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffabe7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_model(rf, plot='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf976d",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<h4>Exercise 2. What is the most important feature? Plot that feature and 'Disease' and check if you can notice any correlation.</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1567ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31236df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/pyc_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ccd906",
   "metadata": {},
   "source": [
    "## Tune a model\n",
    "\n",
    "When a model is created using the `create_model()` function it uses the default hyperparameters to train the model. In order to tune hyperparameters, the `tune_model()` function is used. The `tune_model()` function is a random grid search of hyperparameters over a pre-defined search space. By default, it is set to optimize Accuracy but this can be changed using optimize parameter. For example: tune_model(dt, optimize = 'AUC') will search for the hyperparameters of a Decision Tree Classifier that results in highest AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317a085",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tuned_lr = tune_model(lr, round=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rf = tune_model(rf, round=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761bbc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_knn = tune_model(knn, round=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51848bb8",
   "metadata": {},
   "source": [
    "Notice how the results after tuning have improved:\n",
    "\n",
    "**Logistic regression:** no change </br>\n",
    "**Random forest classification:** from `0.799` to `0.838`  </br>\n",
    "**K nearest neighbors:** from `0.788` to `0.849`  </br>\n",
    "\n",
    "Metrics alone are not the only criteria you should consider when finalizing the best model for production. Other factors to consider include training time, standard deviation of kfolds etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633543c",
   "metadata": {},
   "source": [
    "## Predict on test data\n",
    "\n",
    "Before finalizing the model, it is advisable to perform one final check by predicting the test/hold-out set and reviewing the evaluation metrics. If you look at the information grid after running `setup()`, you will see that 30% (154 samples) of the data has been separated out as test/hold-out sample. All of the evaluation metrics we have seen above are cross validated results based on the training set (70%) only. Now, using our final trained model stored in the tuned_knn variable we will predict against the hold-out sample and evaluate the metrics to see if they are materially different than the CV results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(tuned_knn);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46da5c3",
   "metadata": {},
   "source": [
    "The accuracy of test set is `0.818` compared to `0.849` achieved with the train set. This is not significant difference (since the dataset is quite small) but if there is a large variation between the test and train results, then this would normally indicate over-fitting but could also be due to several other factors and would require further investigation. In this case, we will move forward with finalizing the model and then predicting on unseen data (the 5% that we had separated in the beginning and never exposed to PyCaret)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a97c91",
   "metadata": {},
   "source": [
    "## Finalize model\n",
    "\n",
    "Model finalization is the last step in the experiment. A normal machine learning workflow in PyCaret starts with setup(), followed by comparing all models using compare_models() and shortlisting a few candidate models (based on the metric of interest) to perform several modeling techniques such as hyperparameter tuning. This workflow will eventually lead you to the best model for use in making predictions on new and unseen data. The finalize_model() function fits the model onto the complete dataset including the test/hold-out sample (30% in this case). The purpose of this function is to train the model on the complete dataset before it is deployed in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_knn = finalize_model(tuned_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3891be",
   "metadata": {},
   "source": [
    "## Predict on unseen data\n",
    "\n",
    "Now we can use the test data `X_test` and try to classify them with our trained model. For this we use pycaret's `predict_model()` function and pass the unseen testdata as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc21152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = predict_model(final_knn, data=X_test) # pass the model and test-data as parameters\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc9ff0",
   "metadata": {},
   "source": [
    "The *Label* and *Score* columns are added onto the X_test set. Label is the prediction and score is the probability of the prediction. Notice that predicted results are concatenated to the original dataset while all the transformations are automatically performed in the background. You can also check the metrics on this since you have actual target column `y_test` available. To do that we will use pycaret.utils module. You can do this easily with basic python, but this is good way if you want to check any other metrics (other than accuracy) as well. See example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.utils import check_metric\n",
    "\n",
    "# compare target and predicted labels\n",
    "print(\"Prediction accuracy\", check_metric(y_test, predictions['Label'], metric = 'Accuracy')) \n",
    "print(\"Prediction recall\",check_metric(y_test, predictions['Label'], metric = 'Recall'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315fc9bb",
   "metadata": {},
   "source": [
    "You can use sklearn to plot confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74991c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(final_knn, X=X_test, y_true=y_test, cmap='Blues') # parameters: model, input values X, target values y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c55f0",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "<h4>Exercise 3. Run setup() again but this time without any preprocessing. Then make the same models (logistic regression, random forest and knn, no need to tune) and compare results. </h4>\n",
    "    \n",
    "Use session_id=42\n",
    "</div>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb17b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb79ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/pyc_3.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "376.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
