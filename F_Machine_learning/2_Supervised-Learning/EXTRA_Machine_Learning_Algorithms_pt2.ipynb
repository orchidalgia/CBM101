{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning algorithms and applications on real world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will go through a selection of common supervised ML models used for classification. You will also learn how a typical data scientist might go about to do machine learning on real world datasets. As you'll see, most of the time is actually not spent doing machine learning, but rather in preparing, formatting, cleaning and understanding the data. \n",
    "\n",
    "This notebook is in part based on [this](https://nilearn.github.io/auto_examples/02_decoding/plot_haxby_anova_svm.html#sphx-glr-auto-examples-02-decoding-plot-haxby-anova-svm-py) and [this](https://nilearn.github.io/auto_examples/02_decoding/plot_haxby_different_estimators.html#sphx-glr-auto-examples-02-decoding-plot-haxby-different-estimators-py) tutorial from `nilearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we start by importing the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, plot_roc_curve\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST\n",
    "\n",
    "Above we saw RFs as an example of ensemble learning. The specific technique is known as *bagging* (bootstrap-aggregating). Now we will inspect a model which is based on another related method: *boosting*. Boosting relies on training a model in distinct periods, at each step evaluating which points are more difficult to classify correctly. These difficult points is given extra weight in the next round, incentivising the model to learn those points particularly. \n",
    "\n",
    "XGBOOST is currently not built in to `scikit-learn`, but distributed in its own sklearn-compatibable library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real world application of supervised classification\n",
    "\n",
    "## \"Reading thoughts\" from brain scans\n",
    "\n",
    "The most common type of functional magnetic resonance imaging (fMRI) works by recording magnetic field fluctuations inside brains. Blood oxygenation levels is the factor being measured, and is indicative of regional brain activity. Thoughts and other neural processes yields different patterns of brain activity, and thus theory predicts we should be able to discriminate mental states from fMRI scans. This is known as **neural decoding**.\n",
    "\n",
    "### The Haxby experiment\n",
    "A pioneering experiment in this field was performed by Haxby et al. (2001), in which they scanned subjects under different visual stimuli: pictures of houses and pictures of human faces (among others). Here we will reproduce the experiment. The library `nilearn` has built in everything we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from nilearn.plotting import show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haxby_dataset = datasets.fetch_haxby(fetch_stimuli=True) # only the stimulus, \n",
    "stimulus_info = haxby_dataset.stimuli # list of paths for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The various categories\n",
    "stimulus_info.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can look at the images viewed by the test subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the stimulus \n",
    "for stim_type in stimulus_info:\n",
    "    # skip control images, there are too many\n",
    "    if stim_type != 'controls':\n",
    "\n",
    "        file_names = stimulus_info[stim_type]\n",
    "\n",
    "        fig, axes = plt.subplots(6, 8)\n",
    "        fig.suptitle(stim_type)\n",
    "\n",
    "        for img_path, ax in zip(file_names, axes.ravel()):\n",
    "            ax.imshow(plt.imread(img_path), cmap=plt.cm.gray)\n",
    "\n",
    "        for ax in axes.ravel():\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mask nifti image (3D) is located at: %s' % haxby_dataset.mask)\n",
    "print('Functional nifti image (4D) is located at: %s' %\n",
    "      haxby_dataset.func[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load a volume. \n",
    "from nilearn import image\n",
    "func_filename = haxby_dataset.func[0] # the first test subject\n",
    "func_0 = image.load_img(func_filename).slicer[:,:,:,0] # the first volume in the 4D fMRI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the machine learning jargon, we consider each voxel (3D pixel) as a *feature*. The vast number of voxels in the image poses a challenge (curse of dimensionality), so we want to do some feature selection before training the classifier. But first: \n",
    "\n",
    "#### Exercise 6. how many voxels are in the input volume in total?\n",
    "Tip: use the tab trick on `func_0` to see available attributes and methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex2_6.py\n",
    "print(f\"The zyx-dimensions are {func_0.shape}\")\n",
    "\n",
    "# number of voxels:\n",
    "np.prod(func_0.shape)\n",
    "\n",
    "## Around 160K voxels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to retrieve the labels for each volume (i.e. *what* was the subject looking at?). We have limited ourselves to only two states, so you will have to do some basic data manipulation of the labels. You should carefully read the code below to understand how the data is formatted to fit into the model (Scikit-learn accepts only labels on certain formats, as a one-dimensional numpy array or a pandas Series object).\n",
    "\n",
    "**Tip:** print the intermediate dataframes to understand what each line of code achieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target information as string and give a numerical identifier to each\n",
    "behavioral = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\n",
    "conditions = behavioral['labels']\n",
    "\n",
    "# Restrict the analysis to faces and places\n",
    "condition_mask = behavioral['labels'].isin(['face', 'house']) # we will only consider two states\n",
    "y = conditions[condition_mask]\n",
    "\n",
    "# Confirm that we now have 2 conditions\n",
    "print(y.unique())\n",
    "\n",
    "# Record these as an array of sessions, with fields\n",
    "# for condition (face or house) and run\n",
    "session = behavioral[condition_mask].to_records(index=False)\n",
    "print(session.dtype.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have defined the labels `y`. The next step is to defined the data `X` as a (**samples x features**) matrix. In `nilearn`, masker-objects will apply a mask and extract the data from the region and put into a numpy array, `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "mask_filename = haxby_dataset.mask\n",
    "\n",
    "# For decoding, standardizing is often very important\n",
    "# note that we are also smoothing the data\n",
    "masker = NiftiMasker(mask_img=mask_filename, smoothing_fwhm=4,\n",
    "                     standardize=True, memory=\"nilearn_cache\", memory_level=1)\n",
    "func_filename = haxby_dataset.func[0] # the first test subject\n",
    "X = masker.fit_transform(func_filename) # applying the mask\n",
    "X = X[condition_mask] # selecting only the volumes in condition_mask, i.e. only houses and faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now X is on the form that we typically represent data (rows as samples and columns as features). \n",
    "\n",
    "#### Exercise 7. a) How many samples do we have in X? b) Compare the shape of X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex2_7.py\n",
    "# answer: 216. Matrices are always represented in the order: (rows, columns).\n",
    "# 216 samples, i.e. timepoints\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# now it is in the familiar (nrows x ncols) matrix format for machine learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been prepared for machine learning, most of the work is behind us. In the next step we can select any classifier and put it to the task. We start with the linear support vector machine (SVM) (called SVC in `sklearn`), which is both effective and fast to train. Because we have roughly 40K features, we select only the most informative ones (this increases both training speed and accuracy). an F-test can help us do so, again available from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# Define the dimension reduction to be used.\n",
    "# Here we use a classical univariate feature selection based on F-test,\n",
    "# namely Anova. When doing full-brain analysis, it is better to use\n",
    "# SelectPercentile, keeping 1% of voxels\n",
    "# (because it is independent of the resolution of the data).\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "feature_selection = SelectPercentile(f_classif, percentile=1)\n",
    "\n",
    "# We have our classifier (SVC), our feature selection (SelectPercentile),and now,\n",
    "# we can plug them together in a *pipeline* that performs the two operations\n",
    "# successively:\n",
    "from sklearn.pipeline import make_pipeline\n",
    "anova_svc = make_pipeline(feature_selection, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to select optimal hyperparameters: Cross validation\n",
    "\n",
    "The low **n_samples:n_features** ratio incentivices us to use as many samples as possible for training, while minimizing the test size. We can get an unbiased evaluation of our approach with leave-one-out cross validation. The figure below describes the general process when doing model selection. Here we will deviate slightly by not holding out a final test set.\n",
    "\n",
    "![title](assets/cross_val.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "\n",
    "# Define the cross-validation scheme used for validation.\n",
    "# Here we use a LeaveOneGroupOut cross-validation on the session group\n",
    "# which corresponds to a leave-one-session-out\n",
    "cv = LeaveOneGroupOut()\n",
    "\n",
    "# Compute the prediction accuracy for the different folds (i.e. session)\n",
    "cv_scores = cross_val_score(anova_svc, X, y, cv=cv, groups=session)\n",
    "\n",
    "# Return the corresponding mean prediction accuracy\n",
    "classification_accuracy = cv_scores.mean()\n",
    "\n",
    "# Print the results\n",
    "print(\"Classification accuracy: %.4f / Chance level: %f\" %\n",
    "      (classification_accuracy, 1. / len(y.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB! on study design: because the data is time-continuous, it would be biased if you tried to predict a new time point sitting right in between two already known time points. Because the measurements were divided into *sessions*, we remove a whole session for validation, avoiding the problem of autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which brain area is responsible?\n",
    "\n",
    "So MVPA allows for prediction of mental states from patterns in activity. Now we can ask a biologically interesting question: what part of the brain is responsible for this discrimination of faces and houses? How might you go about to answer such a question? It is surprisingly simple to begin to answer. We can look at the weights assigned by the model. It is not straight-forward to interpret the weights of a SVM, but the information held in `svc.coef_` can be used to weight the importance of the different features. We use the whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_svc.fit(X, y)\n",
    "#y_pred = anova_svc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef = svc.coef_ # the seperating hyperplane is defined by this vector\n",
    "# reverse feature selection\n",
    "coef = feature_selection.inverse_transform(coef) # expand to the full 40K voxels\n",
    "# reverse masking\n",
    "weight_img = masker.inverse_transform(coef) # Reshape the array of coefs back to 3D to match the MRI volume\n",
    "\n",
    "# Use the mean image as a background to avoid relying on anatomical data\n",
    "from nilearn import image\n",
    "mean_img = image.mean_img(func_filename)\n",
    "\n",
    "# Create the figure\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "plot_stat_map(weight_img, mean_img, title='SVM weights')\n",
    "\n",
    "# Saving the results as a Nifti file may also be important\n",
    "#weight_img.to_filename('haxby_face_vs_house.nii')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra exercise. Select another classifier from `sklearn`, and repeat the analysis.  The classifier needs some measure of feature importance, similar to `.coef_` in the SVC.\n",
    "Hint: read the documentation on the `sklearn` webpages before deciding on a classifier. Not all models will have something like `coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example with Naive Bayes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
